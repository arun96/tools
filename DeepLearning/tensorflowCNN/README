Notes:
My network achieves an accuracy of ~97%. In addition to the convolutional and max pooling layers, I have two fully connected layers. The layer is of size 1024, the second is of size 10 - I decided on the size of the first layer through trial and error, and the second has to be of size 10 so as to output the required logits. I chose to have only two layers as the accuracy was already high enough.

All specifications of the project have been met - I used 2000 batches of size 50 to train my data, and am using an Adam Optimizer on the cross entropy loss with 1E-4 as the learning rate. The weight variables are initialized from a normal distribution with mean 0 and standard deviation 0.1, with biases being initialized to 0.1. The first convolutional layer is a 5 x 5 filter with 32 deep and 1 in channel, the second is a 5 x 5 filter with 64 deep and 32 in channels, and both have max pooling with strides = [1, 2, 2, 1]. Between layers, I have the ReLu nonlinearity function.
